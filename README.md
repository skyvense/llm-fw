# LLM Framework

A proxy server for Ollama with monitoring and statistics capabilities.

ä¸€ä¸ªå¸¦æœ‰ç›‘æ§å’Œç»Ÿè®¡åŠŸèƒ½çš„ Ollama ä»£ç†æœåŠ¡å™¨ã€‚

## Features ç‰¹æ€§

- ğŸ”„ Proxy requests to Ollama server
- ğŸ“Š Real-time metrics monitoring
- ğŸ’¾ Request history storage
- ğŸŒ Web UI interface
- âš¡ Streaming response support
- ğŸ“ Configurable through YAML

- ğŸ”„ ä»£ç†è½¬å‘ Ollama æœåŠ¡å™¨è¯·æ±‚
- ğŸ“Š å®æ—¶æŒ‡æ ‡ç›‘æ§
- ğŸ’¾ è¯·æ±‚å†å²å­˜å‚¨
- ğŸŒ Web ç•Œé¢
- âš¡ æµå¼å“åº”æ”¯æŒ
- ğŸ“ é€šè¿‡ YAML é…ç½®

## Prerequisites å‰ç½®æ¡ä»¶

- Go 1.20 or later
- Ollama installed and running
- Git

## Installation å®‰è£…

1. Clone the repository å…‹éš†ä»“åº“:
```bash
git clone https://github.com/yourusername/llm-fw.git
cd llm-fw
```

2. Install dependencies å®‰è£…ä¾èµ–:
```bash
go mod download
```

3. Configure the application é…ç½®åº”ç”¨:
Create a `config.yaml` file in the root directory åœ¨æ ¹ç›®å½•åˆ›å»º `config.yaml` æ–‡ä»¶:
```yaml
server:
  host: "localhost"
  port: 8080

ollama:
  url: "http://localhost:11434"  # Your Ollama server address ä½ çš„ Ollama æœåŠ¡å™¨åœ°å€

storage:
  type: "file"
  path: "data"
```

## Usage ä½¿ç”¨æ–¹æ³•

1. Start the server å¯åŠ¨æœåŠ¡å™¨:
```bash
go run main.go
```

2. Open your browser and navigate to æ‰“å¼€æµè§ˆå™¨è®¿é—®:
```
http://localhost:8080
```

3. Use the proxy server instead of directly accessing Ollama ä½¿ç”¨ä»£ç†æœåŠ¡å™¨è€Œä¸æ˜¯ç›´æ¥è®¿é—® Ollama:
```bash
# Instead of æ›¿ä»£
curl http://localhost:11434/api/generate

# Use ä½¿ç”¨
curl http://localhost:8080/api/generate
```

## Configuration é…ç½®è¯´æ˜

The application can be configured through `config.yaml` in the root directory:

åº”ç”¨å¯ä»¥é€šè¿‡æ ¹ç›®å½•ä¸‹çš„ `config.yaml` è¿›è¡Œé…ç½®ï¼š

```yaml
server:
  host: "localhost"    # Proxy server host ä»£ç†æœåŠ¡å™¨ä¸»æœº
  port: 8080          # Proxy server port ä»£ç†æœåŠ¡å™¨ç«¯å£

ollama:
  url: "http://localhost:11434"  # Ollama server URL Ollama æœåŠ¡å™¨åœ°å€

storage:
  type: "file"        # Storage type (file/memory) å­˜å‚¨ç±»å‹ï¼ˆæ–‡ä»¶/å†…å­˜ï¼‰
  path: "data"        # Storage path for file storage æ–‡ä»¶å­˜å‚¨è·¯å¾„
```

## API Endpoints API æ¥å£

- `GET /api/models` - List available models from Ollama åˆ—å‡º Ollama å¯ç”¨æ¨¡å‹
- `POST /api/generate` - Generate text (proxied to Ollama) ç”Ÿæˆæ–‡æœ¬ï¼ˆä»£ç†åˆ° Ollamaï¼‰
- `GET /api/history` - Get request history è·å–è¯·æ±‚å†å²
- `GET /api/metrics` - Get metrics è·å–æŒ‡æ ‡

## Development å¼€å‘

### Project Structure é¡¹ç›®ç»“æ„

```
llm-fw/
â”œâ”€â”€ api/          # API types and interfaces API ç±»å‹å’Œæ¥å£
â”œâ”€â”€ config/       # Configuration handling é…ç½®å¤„ç†
â”œâ”€â”€ handlers/     # Request handlers è¯·æ±‚å¤„ç†å™¨
â”œâ”€â”€ metrics/      # Metrics collection æŒ‡æ ‡æ”¶é›†
â”œâ”€â”€ routes/       # Route setup è·¯ç”±è®¾ç½®
â”œâ”€â”€ storage/      # Storage implementations å­˜å‚¨å®ç°
â”œâ”€â”€ templates/    # HTML templates HTML æ¨¡æ¿
â”œâ”€â”€ config.yaml   # Application configuration åº”ç”¨é…ç½®
â””â”€â”€ main.go       # Application entry point åº”ç”¨å…¥å£
```

### Adding New Features æ·»åŠ æ–°åŠŸèƒ½

1. Create new types in `api` package åœ¨ `api` åŒ…ä¸­åˆ›å»ºæ–°ç±»å‹
2. Implement handlers in `handlers` package åœ¨ `handlers` åŒ…ä¸­å®ç°å¤„ç†å™¨
3. Add routes in `routes` package åœ¨ `routes` åŒ…ä¸­æ·»åŠ è·¯ç”±
4. Update templates if needed å¦‚æœéœ€è¦ï¼Œæ›´æ–°æ¨¡æ¿

## Contributing è´¡çŒ®

Contributions are welcome! Please feel free to submit a Pull Request.

æ¬¢è¿è´¡çŒ®ï¼è¯·éšæ—¶æäº¤ Pull Requestã€‚

## License è®¸å¯è¯

This project is licensed under the MIT License - see the LICENSE file for details.

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ LICENSE æ–‡ä»¶ã€‚ 